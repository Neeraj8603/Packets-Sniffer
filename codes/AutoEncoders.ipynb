{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.backend as K\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "amex_y17l1wJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetworkAnomalyDetector:\n",
        "    def __init__(self, threshold_percentile=95):\n",
        "        self.threshold_percentile = threshold_percentile\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.models = {}\n",
        "        self.feature_processor = None\n",
        "        self.best_model = None\n",
        "        self.feature_columns = None\n",
        "\n",
        "    def parse_logs(self, file_paths):\n",
        "        data = []\n",
        "        feature_stats = defaultdict(list)\n",
        "\n",
        "        for file_path in file_paths:\n",
        "            with open(file_path, 'r') as file:\n",
        "                entry = {}\n",
        "                for line in file:\n",
        "                    line = line.strip()\n",
        "\n",
        "                    if re.match(r'^[A-Za-z]{3} [A-Za-z]{3} \\d{1,2} \\d{2}:\\d{2}:\\d{2} \\d{4}$', line) or \\\n",
        "                       re.match(r'^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', line):\n",
        "                        if entry:\n",
        "                            data.append(entry)\n",
        "                            entry = {}\n",
        "                        entry['timestamp'] = line\n",
        "\n",
        "                    elif re.match(r'^([0-9a-f]{2}:){5}[0-9a-f]{2}( -> ([0-9a-f]{2}:){5}[0-9a-f]{2})?$', line):\n",
        "                        if ' -> ' in line:\n",
        "                            macs = line.split(' -> ')\n",
        "                            entry['src_mac'] = macs[0]\n",
        "                            entry['dst_mac'] = macs[1]\n",
        "                        else:\n",
        "                            entry['mac'] = line\n",
        "\n",
        "                    elif re.match(r'^(IPv4:|IPv6:)?\\s*([0-9a-f.:]+)(:\\d+)?( -> ([0-9a-f.:]+)(:\\d+)?)?', line):\n",
        "                        if 'IPv4:' in line or 'IPv6:' in line:\n",
        "                            line = line.split(':', 1)[1].strip()\n",
        "                        if ' -> ' in line:\n",
        "                            parts = line.split(' -> ')\n",
        "                            src_part = parts[0].split(':')\n",
        "                            dst_part = parts[1].split(':')\n",
        "                            entry['src_ip'] = src_part[0]\n",
        "                            if len(src_part) > 1:\n",
        "                                entry['src_port'] = src_part[1]\n",
        "                            entry['dst_ip'] = dst_part[0]\n",
        "                            if len(dst_part) > 1:\n",
        "                                entry['dst_port'] = dst_part[1]\n",
        "\n",
        "                    elif re.match(r'(DgmLen:|Length:)\\s*\\d+', line, re.IGNORECASE):\n",
        "                        length_match = re.search(r'\\d+', line)\n",
        "                        if length_match:\n",
        "                            entry['packet_length'] = int(length_match.group())\n",
        "\n",
        "                    elif re.match(r'(PL\\(|Payload:).*([0-9a-f]{2}\\s*)+', line, re.IGNORECASE):\n",
        "                        hex_payload = re.findall(r'[0-9a-f]{2}', line.split(':')[-1])\n",
        "                        if hex_payload:\n",
        "                            entry['payload_sum'] = sum(int(byte, 16) for byte in hex_payload)\n",
        "                            entry['payload_len'] = len(hex_payload)\n",
        "\n",
        "                    elif re.match(r'(Proto:|Protocol:)\\s*\\w+', line, re.IGNORECASE):\n",
        "                        proto_match = re.search(r'\\w+$', line)\n",
        "                        if proto_match:\n",
        "                            entry['protocol'] = proto_match.group().lower()\n",
        "\n",
        "                if entry:\n",
        "                    data.append(entry)\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        self._analyze_features(df)\n",
        "        return df\n",
        "\n",
        "    def _analyze_features(self, df):\n",
        "        numeric_features = []\n",
        "        categorical_features = []\n",
        "\n",
        "        for col in df.columns:\n",
        "            if pd.api.types.is_numeric_dtype(df[col]):\n",
        "                numeric_features.append(col)\n",
        "            elif col not in ['timestamp', 'src_mac', 'dst_mac', 'src_ip', 'dst_ip']:\n",
        "                try:\n",
        "                    df[col] = pd.to_numeric(df[col])\n",
        "                    numeric_features.append(col)\n",
        "                except:\n",
        "                    if df[col].nunique() < 20:\n",
        "                        categorical_features.append(col)\n",
        "\n",
        "        ip_features = []\n",
        "        for col in ['src_ip', 'dst_ip']:\n",
        "            if col in df.columns:\n",
        "                ip_features.append(col)\n",
        "                df[f'{col}_num'] = df[col].apply(self._ip_to_num)\n",
        "                numeric_features.append(f'{col}_num')\n",
        "\n",
        "        numeric_defaults = {col: 0 for col in numeric_features}\n",
        "        df.fillna(numeric_defaults, inplace=True)\n",
        "\n",
        "        if categorical_features:\n",
        "            df = pd.get_dummies(df, columns=categorical_features)\n",
        "            numeric_features.extend([col for col in df.columns if col.endswith('_1')])\n",
        "\n",
        "        self.feature_columns = numeric_features\n",
        "        print(f\"Automatically selected features: {self.feature_columns}\")\n",
        "\n",
        "    def _ip_to_num(self, ip):\n",
        "        if pd.isna(ip):\n",
        "            return 0\n",
        "        try:\n",
        "            if ':' in ip:\n",
        "                return int(ip.replace(':', ''), 16)\n",
        "            else:\n",
        "                return sum(int(part) * (256 ** i) for i, part in enumerate(ip.split('.')[::-1]))\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    def _build_models(self, input_dim):\n",
        "        models = {\n",
        "            'AutoEncoder': self._build_autoencoder(input_dim),\n",
        "            'Sparse AutoEncoder': self._build_sparse_autoencoder(input_dim),\n",
        "            'Variational AutoEncoder': self._build_variational_autoencoder(input_dim),\n",
        "            'Stacked AutoEncoder': self._build_stacked_autoencoder(input_dim)\n",
        "        }\n",
        "        return models\n",
        "\n",
        "    def _build_autoencoder(self, input_dim):\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoded = Dense(32, activation='relu')(input_layer)\n",
        "        decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "        return Model(input_layer, decoded)\n",
        "\n",
        "    def _build_sparse_autoencoder(self, input_dim):\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoded = Dense(32, activation='relu',\n",
        "                       activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
        "        decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "        return Model(input_layer, decoded)\n",
        "\n",
        "    def _build_variational_autoencoder(self, input_dim):\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        h = Dense(32, activation='relu')(input_layer)\n",
        "        z_mean = Dense(16)(h)\n",
        "        z_log_var = Dense(16)(h)\n",
        "\n",
        "        def sampling(args):\n",
        "            z_mean, z_log_var = args\n",
        "            epsilon = K.random_normal(shape=(K.shape(z_mean)[0], 16))\n",
        "            return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "\n",
        "        z = Lambda(sampling)([z_mean, z_log_var])\n",
        "        decoder_h = Dense(32, activation='relu')\n",
        "        decoder_mean = Dense(input_dim, activation='sigmoid')\n",
        "        h_decoded = decoder_h(z)\n",
        "        x_decoded_mean = decoder_mean(h_decoded)\n",
        "        return Model(input_layer, x_decoded_mean)\n",
        "\n",
        "    def _build_stacked_autoencoder(self, input_dim):\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoded = Dense(64, activation='relu')(input_layer)\n",
        "        encoded = Dense(32, activation='relu')(encoded)\n",
        "        decoded = Dense(64, activation='relu')(encoded)\n",
        "        decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "        return Model(input_layer, decoded)\n",
        "\n",
        "    def detect_anomalies(self, reconstructions, original):\n",
        "        anomaly_scores = np.mean(np.abs(original - reconstructions), axis=1)\n",
        "        threshold = np.percentile(anomaly_scores, self.threshold_percentile)\n",
        "        return anomaly_scores > threshold\n",
        "\n",
        "    def train(self, file_paths, epochs=50, batch_size=32):\n",
        "        print(\"Parsing log files...\")\n",
        "        df = self.parse_logs(file_paths)\n",
        "\n",
        "        if not self.feature_columns:\n",
        "            raise ValueError(\"No features detected in the log files\")\n",
        "\n",
        "        X = df[self.feature_columns].values\n",
        "        print(\"Normalizing data...\")\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        print(\"\\nBuilding models...\")\n",
        "        self.models = self._build_models(X_scaled.shape[1])\n",
        "\n",
        "        results = {}\n",
        "        print(\"\\nTraining models...\")\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "            start_time = time.time()\n",
        "            model.fit(X_scaled, X_scaled,\n",
        "                     epochs=epochs,\n",
        "                     batch_size=batch_size,\n",
        "                     shuffle=True,\n",
        "                     validation_split=0.2,\n",
        "                     verbose=0)\n",
        "            training_time = time.time() - start_time\n",
        "\n",
        "            reconstructions = model.predict(X_scaled)\n",
        "            mse = mean_squared_error(X_scaled, reconstructions)\n",
        "            accuracy = 1 - mse\n",
        "            results[name] = {\n",
        "                'Accuracy': accuracy,\n",
        "                'MSE': mse,\n",
        "                'Training Time': training_time\n",
        "            }\n",
        "            print(f\"{name}: Accuracy={accuracy:.4f}, MSE={mse:.6f}, Training Time={training_time:.2f}s\")\n",
        "\n",
        "\n",
        "        self.best_model_name = max(results, key=lambda k: results[k]['Accuracy'])\n",
        "        self.best_model = self.models[self.best_model_name]\n",
        "        print(f\"\\nBest Model: {self.best_model_name} with Accuracy {results[self.best_model_name]['Accuracy']:.4f}\")\n",
        "\n",
        "\n",
        "        print(\"Detecting anomalies...\")\n",
        "        reconstructions = self.best_model.predict(X_scaled)\n",
        "        df['Anomaly'] = self.detect_anomalies(reconstructions, X_scaled)\n",
        "        df['Anomaly_Score'] = np.mean(np.abs(X_scaled - reconstructions), axis=1)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def analyze_new_data(self, file_paths):\n",
        "        if not self.best_model:\n",
        "            raise ValueError(\"Model not trained yet. Call train() first.\")\n",
        "\n",
        "        print(\"Parsing new log files...\")\n",
        "        df = self.parse_logs(file_paths)\n",
        "\n",
        "        if not self.feature_columns:\n",
        "            raise ValueError(\"No features detected in the log files\")\n",
        "\n",
        "        missing_features = set(self.feature_columns) - set(df.columns)\n",
        "        for feat in missing_features:\n",
        "            df[feat] = 0\n",
        "\n",
        "        X = df[self.feature_columns].values\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "\n",
        "        print(\"Detecting anomalies...\")\n",
        "        reconstructions = self.best_model.predict(X_scaled)\n",
        "        df['Anomaly'] = self.detect_anomalies(reconstructions, X_scaled)\n",
        "        df['Anomaly_Score'] = np.mean(np.abs(X_scaled - reconstructions), axis=1)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def save_results(self, df, output_file='network_anomalies.csv'):\n",
        "        output_columns = [col for col in df.columns if not col.endswith('_num')]\n",
        "        df[output_columns].to_csv(output_file, index=False)\n",
        "        print(f\"\\nResults saved to '{output_file}'\")\n",
        "\n",
        "        if 'Anomaly' in df.columns:\n",
        "            print(\"\\nSample anomalies detected:\")\n",
        "            print(df[df['Anomaly']][output_columns].head())\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "2RNGFFH8l8YN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    detector = NetworkAnomalyDetector(threshold_percentile=95)\n",
        "    log_files = [\"logsys1.txt\", \"logts1.txt\",\"logsnort1.txt\"]\n",
        "    df = detector.train(log_files)\n",
        "    detector.save_results(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEVqUG_Qmmnw",
        "outputId": "9e5b65b6-69ee-49a7-def4-cd938bd5f47b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing log files...\n",
            "Automatically selected features: ['src_port', 'dst_port', 'payload_sum', 'payload_len', 'src_ip_num', 'dst_ip_num']\n",
            "Normalizing data...\n",
            "\n",
            "Building models...\n",
            "\n",
            "Training models...\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "AutoEncoder: Accuracy=0.9997, MSE=0.000320, Training Time=28.10s\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Sparse AutoEncoder: Accuracy=0.9994, MSE=0.000611, Training Time=28.63s\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Variational AutoEncoder: Accuracy=0.9999, MSE=0.000132, Training Time=32.87s\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Stacked AutoEncoder: Accuracy=0.9999, MSE=0.000085, Training Time=33.85s\n",
            "\n",
            "Best Model: Stacked AutoEncoder with Accuracy 0.9999\n",
            "Detecting anomalies...\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "Results saved to 'network_anomalies.csv'\n",
            "\n",
            "Sample anomalies detected:\n",
            "                   timestamp            src_mac            dst_mac  \\\n",
            "13  Thu Mar 20 23:55:14 2025  6a:61:ab:9d:ff:79  a0:59:50:93:56:70   \n",
            "15  Thu Mar 20 23:55:19 2025  6a:61:ab:9d:ff:79  a0:59:50:93:56:70   \n",
            "17  Thu Mar 20 23:55:21 2025  6a:61:ab:9d:ff:79  a0:59:50:93:56:70   \n",
            "24  Thu Mar 20 23:55:24 2025  6a:61:ab:9d:ff:79  ff:ff:ff:ff:ff:ff   \n",
            "25  Thu Mar 20 23:55:24 2025  a0:59:50:93:56:70  6a:61:ab:9d:ff:79   \n",
            "\n",
            "             src_ip  src_port           dst_ip  dst_port  payload_sum  \\\n",
            "13    140.82.112.25     443.0  192.168.135.162   54129.0          0.0   \n",
            "15    108.159.15.29     443.0  192.168.135.162   54174.0          0.0   \n",
            "17     104.18.26.48     443.0  192.168.135.162   54175.0          0.0   \n",
            "24  255.121.192.168       0.0       135.90.0.0       0.0       4620.0   \n",
            "25   86.112.192.168       0.0   135.162.106.97       0.0       4439.0   \n",
            "\n",
            "    payload_len  Anomaly  Anomaly_Score  \n",
            "13          0.0     True       0.013348  \n",
            "15          0.0     True       0.011956  \n",
            "17          0.0     True       0.012168  \n",
            "24         60.0     True       0.018194  \n",
            "25         60.0     True       0.034354  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}