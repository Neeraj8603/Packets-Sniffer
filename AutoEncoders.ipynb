{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.backend as K\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "a6VU4cDsKlR3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetworkAnomalyDetector:\n",
        "    def __init__(self):\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.models = {}\n",
        "        self.feature_processor = None\n",
        "        self.best_model = None\n",
        "        self.feature_columns = None\n",
        "\n",
        "    def parse_logs(self, file_paths):\n",
        "        data = []\n",
        "        feature_stats = defaultdict(list)\n",
        "\n",
        "        for file_path in file_paths:\n",
        "            with open(file_path, 'r') as file:\n",
        "                entry = {}\n",
        "                for line in file:\n",
        "                    line = line.strip()\n",
        "\n",
        "                    if re.match(r'^[A-Za-z]{3} [A-Za-z]{3} \\d{1,2} \\d{2}:\\d{2}:\\d{2} \\d{4}$', line) or \\\n",
        "                       re.match(r'^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', line):\n",
        "                        if entry:\n",
        "                            data.append(entry)\n",
        "                            entry = {}\n",
        "                        entry['timestamp'] = line\n",
        "\n",
        "                    elif re.match(r'^([0-9a-f]{2}:){5}[0-9a-f]{2}( -> ([0-9a-f]{2}:){5}[0-9a-f]{2})?$', line):\n",
        "                        if ' -> ' in line:\n",
        "                            macs = line.split(' -> ')\n",
        "                            entry['src_mac'] = macs[0]\n",
        "                            entry['dst_mac'] = macs[1]\n",
        "                        else:\n",
        "                            entry['mac'] = line\n",
        "\n",
        "                    elif re.match(r'^(IPv4:|IPv6:)?\\s*([0-9a-f.:]+)(:\\d+)?( -> ([0-9a-f.:]+)(:\\d+)?)?', line):\n",
        "                        if 'IPv4:' in line or 'IPv6:' in line:\n",
        "                            line = line.split(':', 1)[1].strip()\n",
        "                        if ' -> ' in line:\n",
        "                            parts = line.split(' -> ')\n",
        "                            src_part = parts[0].split(':')\n",
        "                            dst_part = parts[1].split(':')\n",
        "                            entry['src_ip'] = src_part[0]\n",
        "                            if len(src_part) > 1:\n",
        "                                entry['src_port'] = src_part[1]\n",
        "                            entry['dst_ip'] = dst_part[0]\n",
        "                            if len(dst_part) > 1:\n",
        "                                entry['dst_port'] = dst_part[1]\n",
        "\n",
        "                    elif re.match(r'(DgmLen:|Length:)\\s*\\d+', line, re.IGNORECASE):\n",
        "                        length_match = re.search(r'\\d+', line)\n",
        "                        if length_match:\n",
        "                            entry['packet_length'] = int(length_match.group())\n",
        "\n",
        "                    elif re.match(r'(PL\\(|Payload:).*([0-9a-f]{2}\\s*)+', line, re.IGNORECASE):\n",
        "                        hex_payload = re.findall(r'[0-9a-f]{2}', line.split(':')[-1])\n",
        "                        if hex_payload:\n",
        "                            entry['payload_sum'] = sum(int(byte, 16) for byte in hex_payload)\n",
        "                            entry['payload_len'] = len(hex_payload)\n",
        "\n",
        "                    elif re.match(r'(Proto:|Protocol:)\\s*\\w+', line, re.IGNORECASE):\n",
        "                        proto_match = re.search(r'\\w+$', line)\n",
        "                        if proto_match:\n",
        "                            entry['protocol'] = proto_match.group().lower()\n",
        "\n",
        "                if entry:\n",
        "                    data.append(entry)\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        self._analyze_features(df)\n",
        "        return df\n",
        "\n",
        "    def _analyze_features(self, df):\n",
        "        numeric_features = []\n",
        "        categorical_features = []\n",
        "\n",
        "        for col in df.columns:\n",
        "            if pd.api.types.is_numeric_dtype(df[col]):\n",
        "                numeric_features.append(col)\n",
        "            elif col not in ['timestamp', 'src_mac', 'dst_mac', 'src_ip', 'dst_ip']:\n",
        "                try:\n",
        "                    df[col] = pd.to_numeric(df[col])\n",
        "                    numeric_features.append(col)\n",
        "                except:\n",
        "                    if df[col].nunique() < 20:\n",
        "                        categorical_features.append(col)\n",
        "\n",
        "        ip_features = []\n",
        "        for col in ['src_ip', 'dst_ip']:\n",
        "            if col in df.columns:\n",
        "                ip_features.append(col)\n",
        "                df[f'{col}_num'] = df[col].apply(self._ip_to_num)\n",
        "                numeric_features.append(f'{col}_num')\n",
        "\n",
        "        numeric_defaults = {col: 0 for col in numeric_features}\n",
        "        df.fillna(numeric_defaults, inplace=True)\n",
        "\n",
        "        if categorical_features:\n",
        "            df = pd.get_dummies(df, columns=categorical_features)\n",
        "            numeric_features.extend([col for col in df.columns if col.endswith('_1')])\n",
        "\n",
        "        self.feature_columns = numeric_features\n",
        "        print(f\"Automatically selected features: {self.feature_columns}\")\n",
        "\n",
        "    def _ip_to_num(self, ip):\n",
        "        if pd.isna(ip):\n",
        "            return 0\n",
        "        try:\n",
        "            if ':' in ip:\n",
        "                return int(ip.replace(':', ''), 16)\n",
        "            else:\n",
        "                return sum(int(part) * (256 ** i) for i, part in enumerate(ip.split('.')[::-1]))\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    def _build_models(self, input_dim):\n",
        "        models = {\n",
        "            'AutoEncoder': self._build_autoencoder(input_dim),\n",
        "            'Sparse AutoEncoder': self._build_sparse_autoencoder(input_dim),\n",
        "            'Variational AutoEncoder': self._build_variational_autoencoder(input_dim),\n",
        "            'Stacked AutoEncoder': self._build_stacked_autoencoder(input_dim)\n",
        "        }\n",
        "        return models\n",
        "\n",
        "    def _build_autoencoder(self, input_dim):\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoded = Dense(32, activation='relu')(input_layer)\n",
        "        decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "        return Model(input_layer, decoded)\n",
        "\n",
        "    def _build_sparse_autoencoder(self, input_dim):\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoded = Dense(32, activation='relu',\n",
        "                       activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
        "        decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "        return Model(input_layer, decoded)\n",
        "\n",
        "    def _build_variational_autoencoder(self, input_dim):\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        h = Dense(32, activation='relu')(input_layer)\n",
        "        z_mean = Dense(16)(h)\n",
        "        z_log_var = Dense(16)(h)\n",
        "\n",
        "        def sampling(args):\n",
        "            z_mean, z_log_var = args\n",
        "            epsilon = K.random_normal(shape=(K.shape(z_mean)[0], 16))\n",
        "            return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "\n",
        "        z = Lambda(sampling)([z_mean, z_log_var])\n",
        "        decoder_h = Dense(32, activation='relu')\n",
        "        decoder_mean = Dense(input_dim, activation='sigmoid')\n",
        "        h_decoded = decoder_h(z)\n",
        "        x_decoded_mean = decoder_mean(h_decoded)\n",
        "        return Model(input_layer, x_decoded_mean)\n",
        "\n",
        "    def _build_stacked_autoencoder(self, input_dim):\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoded = Dense(64, activation='relu')(input_layer)\n",
        "        encoded = Dense(32, activation='relu')(encoded)\n",
        "        decoded = Dense(64, activation='relu')(encoded)\n",
        "        decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "        return Model(input_layer, decoded)\n",
        "\n",
        "    def train(self, file_paths, epochs=50, batch_size=32):\n",
        "        print(\"Parsing log files...\")\n",
        "        df = self.parse_logs(file_paths)\n",
        "\n",
        "        if not self.feature_columns:\n",
        "            raise ValueError(\"No features detected in the log files\")\n",
        "\n",
        "        X = df[self.feature_columns].values\n",
        "        print(\"Normalizing data...\")\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        print(\"\\nBuilding models...\")\n",
        "        self.models = self._build_models(X_scaled.shape[1])\n",
        "\n",
        "        results = {}\n",
        "        print(\"\\nTraining models...\")\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "            start_time = time.time()\n",
        "            model.fit(X_scaled, X_scaled,\n",
        "                     epochs=epochs,\n",
        "                     batch_size=batch_size,\n",
        "                     shuffle=True,\n",
        "                     validation_split=0.2,\n",
        "                     verbose=0)\n",
        "            training_time = time.time() - start_time\n",
        "\n",
        "            reconstructions = model.predict(X_scaled)\n",
        "            mse = mean_squared_error(X_scaled, reconstructions)\n",
        "            accuracy = 1 - mse\n",
        "            results[name] = {\n",
        "                'Accuracy': accuracy,\n",
        "                'MSE': mse,\n",
        "                'Training Time': training_time\n",
        "            }\n",
        "            print(f\"{name}: Accuracy={accuracy:.4f}, MSE={mse:.6f}, Training Time={training_time:.2f}s\")\n",
        "\n",
        "        self.best_model_name = max(results, key=lambda k: results[k]['Accuracy'])\n",
        "        self.best_model = self.models[self.best_model_name]\n",
        "        print(f\"\\nBest Model: {self.best_model_name} with Accuracy {results[self.best_model_name]['Accuracy']:.4f}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def analyze_new_data(self, file_paths):\n",
        "        if not self.best_model:\n",
        "            raise ValueError(\"Model not trained yet. Call train() first.\")\n",
        "\n",
        "        print(\"Parsing new log files...\")\n",
        "        df = self.parse_logs(file_paths)\n",
        "\n",
        "        if not self.feature_columns:\n",
        "            raise ValueError(\"No features detected in the log files\")\n",
        "\n",
        "        missing_features = set(self.feature_columns) - set(df.columns)\n",
        "        for feat in missing_features:\n",
        "            df[feat] = 0\n",
        "\n",
        "        X = df[self.feature_columns].values\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def save_results(self, df, output_file='network_anomalies.csv'):\n",
        "        output_columns = [col for col in df.columns if not col.endswith('_num')]\n",
        "        df[output_columns].to_csv(output_file, index=False)\n",
        "        print(f\"\\nResults saved to '{output_file}'\")\n",
        "        return df"
      ],
      "metadata": {
        "id": "bqzBKwrZKpn0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    detector = NetworkAnomalyDetector()\n",
        "    log_files = [\"logsys1.txt\", \"logts1.txt\",\"logsnort1.txt\"]\n",
        "    df = detector.train(log_files)\n",
        "    detector.save_results(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDa2DTKHKs_m",
        "outputId": "e3020314-1051-4dd9-aad7-ff0fcbbf78a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing log files...\n",
            "Automatically selected features: ['src_port', 'dst_port', 'payload_sum', 'payload_len', 'src_ip_num', 'dst_ip_num']\n",
            "Normalizing data...\n",
            "\n",
            "Building models...\n",
            "\n",
            "Training models...\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "AutoEncoder: Accuracy=0.9997, MSE=0.000312, Training Time=32.54s\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Sparse AutoEncoder: Accuracy=0.9994, MSE=0.000619, Training Time=34.37s\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Variational AutoEncoder: Accuracy=0.9999, MSE=0.000111, Training Time=36.75s\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Stacked AutoEncoder: Accuracy=0.9999, MSE=0.000096, Training Time=35.83s\n",
            "\n",
            "Best Model: Stacked AutoEncoder with Accuracy 0.9999\n",
            "\n",
            "Results saved to 'network_anomalies.csv'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}