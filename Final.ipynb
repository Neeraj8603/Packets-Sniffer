{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.backend as K\n",
        "import os"
      ],
      "metadata": {
        "id": "6omJE3KCo0kx"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_logs(file_paths):\n",
        "    \"\"\"Parse network log files while preserving all original data exactly.\"\"\"\n",
        "    data = []\n",
        "    for file_path in file_paths:\n",
        "        with open(file_path, 'r') as file:\n",
        "            entry = {}\n",
        "            for line in file:\n",
        "                line = line.strip()\n",
        "\n",
        "                # Match timestamp (e.g., \"Thu Mar 20 23:55:13 2025\")\n",
        "                if re.match(r'^[A-Za-z]{3} [A-Za-z]{3} \\d{1,2} \\d{2}:\\d{2}:\\d{2} \\d{4}$', line):\n",
        "                    if entry:\n",
        "                        data.append(entry)\n",
        "                        entry = {}\n",
        "                    entry['timestamp'] = line\n",
        "\n",
        "                # Match MAC addresses (e.g., \"a0:59:50:93:56:70 -> 6a:61:ab:9d:ff:79\")\n",
        "                elif re.match(r'^[0-9a-f]{2}(:[0-9a-f]{2}){5} -> [0-9a-f]{2}(:[0-9a-f]{2}){5}$', line):\n",
        "                    macs = line.split(' -> ')\n",
        "                    entry['src_mac'] = macs[0]\n",
        "                    entry['dst_mac'] = macs[1]\n",
        "\n",
        "                # Match IPv4 information (e.g., \"IPv4: 192.168.135.162:61426 -> 192.168.135.90:53\")\n",
        "                elif 'IPv4:' in line:\n",
        "                    ip_match = re.search(r'IPv4: (\\d+\\.\\d+\\.\\d+\\.\\d+):(\\d+) -> (\\d+\\.\\d+\\.\\d+\\.\\d+):(\\d+)', line)\n",
        "                    if ip_match:\n",
        "                        entry['src_ip'], entry['src_port'], entry['dst_ip'], entry['dst_port'] = ip_match.groups()\n",
        "\n",
        "                # Match packet length information\n",
        "                elif 'DgmLen:' in line:\n",
        "                    length_match = re.search(r'DgmLen:(\\d+)', line)\n",
        "                    if length_match:\n",
        "                        entry['packet_length'] = int(length_match.group(1))\n",
        "\n",
        "                # Match payload if available\n",
        "                elif 'PL(' in line and '):' in line:\n",
        "                    hex_payload = line.split('):')[1].strip().split()\n",
        "                    if hex_payload:\n",
        "                        entry['payload_sum'] = sum(int(byte, 16) for byte in hex_payload if len(byte) == 2)\n",
        "                        entry['payload_len'] = len(hex_payload)\n",
        "\n",
        "            if entry:\n",
        "                data.append(entry)\n",
        "\n",
        "    # Create DataFrame while preserving all original values\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Only fill missing numeric values, leave other fields as-is\n",
        "    numeric_defaults = {\n",
        "        'packet_length': 0,\n",
        "        'payload_sum': 0,\n",
        "        'payload_len': 0,\n",
        "        'src_port': '0',  # Keep as string to match original format\n",
        "        'dst_port': '0'\n",
        "    }\n",
        "    df.fillna(numeric_defaults, inplace=True)\n",
        "\n",
        "    # Create numerical versions of IPs for modeling while preserving originals\n",
        "    def ip_to_num(ip):\n",
        "        if pd.isna(ip):\n",
        "            return 0\n",
        "        try:\n",
        "            return sum(int(part) * (256 ** i) for i, part in enumerate(ip.split('.')[::-1]))\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    df['src_ip_num'] = df['src_ip'].apply(ip_to_num)\n",
        "    df['dst_ip_num'] = df['dst_ip'].apply(ip_to_num)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "IVTR8H_xo6YB"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_autoencoder(input_dim):\n",
        "    \"\"\"Build a basic autoencoder model.\"\"\"\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(32, activation='relu')(input_layer)\n",
        "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "    autoencoder = Model(input_layer, decoded)\n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "9eijHBXtpBFA"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_sparse_autoencoder(input_dim):\n",
        "    \"\"\"Build a sparse autoencoder with L1 regularization.\"\"\"\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(32, activation='relu',\n",
        "                   activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
        "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "    autoencoder = Model(input_layer, decoded)\n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "MOEdQf0bpHLf"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_variational_autoencoder(input_dim):\n",
        "    \"\"\"Build a variational autoencoder.\"\"\"\n",
        "    # Encoder\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    h = Dense(32, activation='relu')(input_layer)\n",
        "    z_mean = Dense(16)(h)\n",
        "    z_log_var = Dense(16)(h)\n",
        "\n",
        "    # Sampling function\n",
        "    def sampling(args):\n",
        "        z_mean, z_log_var = args\n",
        "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], 16))\n",
        "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "\n",
        "    z = Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "    # Decoder\n",
        "    decoder_h = Dense(32, activation='relu')\n",
        "    decoder_mean = Dense(input_dim, activation='sigmoid')\n",
        "    h_decoded = decoder_h(z)\n",
        "    x_decoded_mean = decoder_mean(h_decoded)\n",
        "\n",
        "    vae = Model(input_layer, x_decoded_mean)\n",
        "    return vae"
      ],
      "metadata": {
        "id": "N0E8hOZ-pOIq"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_stacked_autoencoder(input_dim):\n",
        "    \"\"\"Build a stacked autoencoder with multiple layers.\"\"\"\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(64, activation='relu')(input_layer)\n",
        "    encoded = Dense(32, activation='relu')(encoded)\n",
        "    decoded = Dense(64, activation='relu')(encoded)\n",
        "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "    autoencoder = Model(input_layer, decoded)\n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "kKZI4jf8pSoC"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_anomalies(reconstructions, original, threshold_percentile=95):\n",
        "    \"\"\"Detect anomalies based on reconstruction error.\"\"\"\n",
        "    anomaly_scores = np.mean(np.abs(original - reconstructions), axis=1)\n",
        "    threshold = np.percentile(anomaly_scores, threshold_percentile)\n",
        "    return anomaly_scores > threshold"
      ],
      "metadata": {
        "id": "k1JmpgUjpWXT"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    # List of log files to process\n",
        "    log_files = [\"logsys1.txt\"]  # Replace with your actual log file(s)\n",
        "\n",
        "    # Parse logs while preserving original IPs and all data\n",
        "    print(\"Parsing log files...\")\n",
        "    df = parse_logs(log_files)\n",
        "\n",
        "    # Select features for anomaly detection (using numerical IPs)\n",
        "    features = ['packet_length', 'src_ip_num', 'dst_ip_num', 'payload_sum', 'payload_len']\n",
        "    X = df[features].values\n",
        "\n",
        "    # Normalize features\n",
        "    print(\"Normalizing data...\")\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Build and compare models\n",
        "    models = {\n",
        "        'AutoEncoder': build_autoencoder(X_scaled.shape[1]),\n",
        "        'Sparse AutoEncoder': build_sparse_autoencoder(X_scaled.shape[1]),\n",
        "        'Variational AutoEncoder': build_variational_autoencoder(X_scaled.shape[1]),\n",
        "        'Stacked AutoEncoder': build_stacked_autoencoder(X_scaled.shape[1])\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    print(\"\\nTraining models...\")\n",
        "\n",
        "    for name, model in models.items():\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "        start_time = time.time()\n",
        "        model.fit(X_scaled, X_scaled,\n",
        "                 epochs=50,\n",
        "                 batch_size=8,\n",
        "                 shuffle=True,\n",
        "                 validation_split=0.2,\n",
        "                 verbose=0)\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        reconstructions = model.predict(X_scaled)\n",
        "        mse = mean_squared_error(X_scaled, reconstructions)\n",
        "        accuracy = 1 - mse\n",
        "        results[name] = {\n",
        "            'Accuracy': accuracy,\n",
        "            'MSE': mse,\n",
        "            'Training Time': training_time\n",
        "        }\n",
        "        print(f\"{name}: Accuracy={accuracy:.4f}, MSE={mse:.6f}, Training Time={training_time:.2f}s\")\n",
        "\n",
        "    # Select best model\n",
        "    best_model_name = max(results, key=lambda k: results[k]['Accuracy'])\n",
        "    best_model = models[best_model_name]\n",
        "    print(f\"\\nBest Model: {best_model_name} with Accuracy {results[best_model_name]['Accuracy']:.4f}\")\n",
        "\n",
        "    # Detect anomalies with best model\n",
        "    print(\"Detecting anomalies...\")\n",
        "    reconstructions = best_model.predict(X_scaled)\n",
        "    df['Anomaly'] = detect_anomalies(reconstructions, X_scaled)\n",
        "\n",
        "    # Save results with original IPs and all data\n",
        "    output_columns = ['timestamp', 'src_mac', 'dst_mac', 'src_ip', 'dst_ip',\n",
        "                     'src_port', 'dst_port', 'packet_length', 'payload_len',\n",
        "                     'payload_sum', 'Anomaly']\n",
        "\n",
        "    # Ensure all columns exist (in case some logs didn't have all fields)\n",
        "    for col in output_columns:\n",
        "        if col not in df.columns:\n",
        "            df[col] = None\n",
        "\n",
        "    df[output_columns].to_csv('network_anomalies.csv', index=False)\n",
        "    print(\"\\nAnomaly detection complete. Results saved to 'network_anomalies.csv'\")\n",
        "\n",
        "    # Show some detected anomalies\n",
        "    print(\"\\nSample anomalies detected:\")\n",
        "    print(df[df['Anomaly']][output_columns].head())\n",
        "\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtOf7cBFpdNB",
        "outputId": "83a343d9-6547-4525-9bbd-5726ef93fcbb"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing log files...\n",
            "Normalizing data...\n",
            "\n",
            "Training models...\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "AutoEncoder: Accuracy=0.9995, MSE=0.000527, Training Time=118.49s\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Sparse AutoEncoder: Accuracy=0.9999, MSE=0.000076, Training Time=115.43s\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Variational AutoEncoder: Accuracy=1.0000, MSE=0.000050, Training Time=116.44s\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Stacked AutoEncoder: Accuracy=1.0000, MSE=0.000029, Training Time=118.00s\n",
            "\n",
            "Best Model: Stacked AutoEncoder with Accuracy 1.0000\n",
            "Detecting anomalies...\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "Anomaly detection complete. Results saved to 'network_anomalies.csv'\n",
            "\n",
            "Sample anomalies detected:\n",
            "                    timestamp            src_mac            dst_mac  \\\n",
            "49   Thu Mar 20 23:55:31 2025  a0:59:50:93:56:70  6a:61:ab:9d:ff:79   \n",
            "57   Thu Mar 20 23:55:31 2025  a0:59:50:93:56:70  6a:61:ab:9d:ff:79   \n",
            "100  Thu Mar 20 23:56:03 2025  a0:59:50:93:56:70  6a:61:ab:9d:ff:79   \n",
            "102  Thu Mar 20 23:56:03 2025  a0:59:50:93:56:70  6a:61:ab:9d:ff:79   \n",
            "103  Thu Mar 20 23:56:03 2025  a0:59:50:93:56:70  6a:61:ab:9d:ff:79   \n",
            "\n",
            "              src_ip         dst_ip src_port dst_port  packet_length  \\\n",
            "49   192.168.135.162  140.82.112.26    54178      443          579.0   \n",
            "57   192.168.135.162  140.82.112.26    54178      443         1218.0   \n",
            "100  192.168.135.162   104.18.26.48    54175      443         1028.0   \n",
            "102  192.168.135.162   104.18.26.48    54175      443          627.0   \n",
            "103  192.168.135.162  140.82.112.26    54178      443          628.0   \n",
            "\n",
            "     payload_len  payload_sum  Anomaly  \n",
            "49         579.0      65811.0     True  \n",
            "57        1218.0     154932.0     True  \n",
            "100       1028.0     129531.0     True  \n",
            "102        627.0      78932.0     True  \n",
            "103        628.0      76472.0     True  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}