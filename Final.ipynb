{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.backend as K\n",
        "import os"
      ],
      "metadata": {
        "id": "6omJE3KCo0kx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_logs(file_paths):\n",
        "    \"\"\"Parse network log files while preserving all original data exactly.\"\"\"\n",
        "    data = []\n",
        "    for file_path in file_paths:\n",
        "        with open(file_path, 'r') as file:\n",
        "            entry = {}\n",
        "            for line in file:\n",
        "                line = line.strip()\n",
        "\n",
        "                # Match timestamp (e.g., \"Thu Mar 20 23:55:13 2025\")\n",
        "                if re.match(r'^[A-Za-z]{3} [A-Za-z]{3} \\d{1,2} \\d{2}:\\d{2}:\\d{2} \\d{4}$', line):\n",
        "                    if entry:\n",
        "                        data.append(entry)\n",
        "                        entry = {}\n",
        "                    entry['timestamp'] = line\n",
        "\n",
        "                # Match MAC addresses (e.g., \"a0:59:50:93:56:70 -> 6a:61:ab:9d:ff:79\")\n",
        "                elif re.match(r'^[0-9a-f]{2}(:[0-9a-f]{2}){5} -> [0-9a-f]{2}(:[0-9a-f]{2}){5}$', line):\n",
        "                    macs = line.split(' -> ')\n",
        "                    entry['src_mac'] = macs[0]\n",
        "                    entry['dst_mac'] = macs[1]\n",
        "\n",
        "                # Match IPv4 information (e.g., \"IPv4: 192.168.135.162:61426 -> 192.168.135.90:53\")\n",
        "                elif 'IPv4:' in line:\n",
        "                    ip_match = re.search(r'IPv4: (\\d+\\.\\d+\\.\\d+\\.\\d+):(\\d+) -> (\\d+\\.\\d+\\.\\d+\\.\\d+):(\\d+)', line)\n",
        "                    if ip_match:\n",
        "                        entry['src_ip'], entry['src_port'], entry['dst_ip'], entry['dst_port'] = ip_match.groups()\n",
        "\n",
        "                # Match packet length information\n",
        "                elif 'DgmLen:' in line:\n",
        "                    length_match = re.search(r'DgmLen:(\\d+)', line)\n",
        "                    if length_match:\n",
        "                        entry['packet_length'] = int(length_match.group(1))\n",
        "\n",
        "                # Match payload if available\n",
        "                elif 'PL(' in line and '):' in line:\n",
        "                    hex_payload = line.split('):')[1].strip().split()\n",
        "                    if hex_payload:\n",
        "                        entry['payload_sum'] = sum(int(byte, 16) for byte in hex_payload if len(byte) == 2)\n",
        "                        entry['payload_len'] = len(hex_payload)\n",
        "\n",
        "            if entry:\n",
        "                data.append(entry)\n",
        "\n",
        "    # Create DataFrame while preserving all original values\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Only fill missing numeric values, leave other fields as-is\n",
        "    numeric_defaults = {\n",
        "        'packet_length': 0,\n",
        "        'payload_sum': 0,\n",
        "        'payload_len': 0,\n",
        "        'src_port': '0',  # Keep as string to match original format\n",
        "        'dst_port': '0'\n",
        "    }\n",
        "    df.fillna(numeric_defaults, inplace=True)\n",
        "\n",
        "    # Create numerical versions of IPs for modeling while preserving originals\n",
        "    def ip_to_num(ip):\n",
        "        if pd.isna(ip):\n",
        "            return 0\n",
        "        try:\n",
        "            return sum(int(part) * (256 ** i) for i, part in enumerate(ip.split('.')[::-1]))\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    df['src_ip_num'] = df['src_ip'].apply(ip_to_num)\n",
        "    df['dst_ip_num'] = df['dst_ip'].apply(ip_to_num)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "IVTR8H_xo6YB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_autoencoder(input_dim):\n",
        "    \"\"\"Build a basic autoencoder model.\"\"\"\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(32, activation='relu')(input_layer)\n",
        "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "    autoencoder = Model(input_layer, decoded)\n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "9eijHBXtpBFA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_sparse_autoencoder(input_dim):\n",
        "    \"\"\"Build a sparse autoencoder with L1 regularization.\"\"\"\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(32, activation='relu',\n",
        "                   activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
        "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "    autoencoder = Model(input_layer, decoded)\n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "MOEdQf0bpHLf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_variational_autoencoder(input_dim):\n",
        "    \"\"\"Build a variational autoencoder.\"\"\"\n",
        "    # Encoder\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    h = Dense(32, activation='relu')(input_layer)\n",
        "    z_mean = Dense(16)(h)\n",
        "    z_log_var = Dense(16)(h)\n",
        "\n",
        "    # Sampling function\n",
        "    def sampling(args):\n",
        "        z_mean, z_log_var = args\n",
        "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], 16))\n",
        "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "\n",
        "    z = Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "    # Decoder\n",
        "    decoder_h = Dense(32, activation='relu')\n",
        "    decoder_mean = Dense(input_dim, activation='sigmoid')\n",
        "    h_decoded = decoder_h(z)\n",
        "    x_decoded_mean = decoder_mean(h_decoded)\n",
        "\n",
        "    vae = Model(input_layer, x_decoded_mean)\n",
        "    return vae"
      ],
      "metadata": {
        "id": "N0E8hOZ-pOIq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_stacked_autoencoder(input_dim):\n",
        "    \"\"\"Build a stacked autoencoder with multiple layers.\"\"\"\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(64, activation='relu')(input_layer)\n",
        "    encoded = Dense(32, activation='relu')(encoded)\n",
        "    decoded = Dense(64, activation='relu')(encoded)\n",
        "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "    autoencoder = Model(input_layer, decoded)\n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "kKZI4jf8pSoC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_anomalies(reconstructions, original, threshold_percentile=95):\n",
        "    \"\"\"Detect anomalies based on reconstruction error.\"\"\"\n",
        "    anomaly_scores = np.mean(np.abs(original - reconstructions), axis=1)\n",
        "    threshold = np.percentile(anomaly_scores, threshold_percentile)\n",
        "    return anomaly_scores > threshold"
      ],
      "metadata": {
        "id": "k1JmpgUjpWXT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.backend as K\n",
        "from collections import defaultdict\n",
        "\n",
        "class NetworkAnomalyDetector:\n",
        "    def __init__(self, threshold_percentile=95):\n",
        "        self.threshold_percentile = threshold_percentile\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.models = {}\n",
        "        self.feature_processor = None\n",
        "        self.best_model = None\n",
        "        self.feature_columns = None\n",
        "\n",
        "    def parse_logs(self, file_paths):\n",
        "        \"\"\"Universal log parser that automatically detects and extracts features\"\"\"\n",
        "        data = []\n",
        "        feature_stats = defaultdict(list)\n",
        "\n",
        "        for file_path in file_paths:\n",
        "            with open(file_path, 'r') as file:\n",
        "                entry = {}\n",
        "                for line in file:\n",
        "                    line = line.strip()\n",
        "\n",
        "                    # Timestamp detection (flexible format)\n",
        "                    if re.match(r'^[A-Za-z]{3} [A-Za-z]{3} \\d{1,2} \\d{2}:\\d{2}:\\d{2} \\d{4}$', line) or \\\n",
        "                       re.match(r'^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', line):\n",
        "                        if entry:\n",
        "                            data.append(entry)\n",
        "                            entry = {}\n",
        "                        entry['timestamp'] = line\n",
        "\n",
        "                    # MAC address detection\n",
        "                    elif re.match(r'^([0-9a-f]{2}:){5}[0-9a-f]{2}( -> ([0-9a-f]{2}:){5}[0-9a-f]{2})?$', line):\n",
        "                        if ' -> ' in line:\n",
        "                            macs = line.split(' -> ')\n",
        "                            entry['src_mac'] = macs[0]\n",
        "                            entry['dst_mac'] = macs[1]\n",
        "                        else:\n",
        "                            entry['mac'] = line\n",
        "\n",
        "                    # IP address detection (IPv4 and IPv6)\n",
        "                    elif re.match(r'^(IPv4:|IPv6:)?\\s*([0-9a-f.:]+)(:\\d+)?( -> ([0-9a-f.:]+)(:\\d+)?)?', line):\n",
        "                        if 'IPv4:' in line or 'IPv6:' in line:\n",
        "                            line = line.split(':', 1)[1].strip()\n",
        "                        if ' -> ' in line:\n",
        "                            parts = line.split(' -> ')\n",
        "                            src_part = parts[0].split(':')\n",
        "                            dst_part = parts[1].split(':')\n",
        "                            entry['src_ip'] = src_part[0]\n",
        "                            if len(src_part) > 1:\n",
        "                                entry['src_port'] = src_part[1]\n",
        "                            entry['dst_ip'] = dst_part[0]\n",
        "                            if len(dst_part) > 1:\n",
        "                                entry['dst_port'] = dst_part[1]\n",
        "                        else:\n",
        "                            ip_part = line.split(':')\n",
        "                            entry['ip'] = ip_part[0]\n",
        "                            if len(ip_part) > 1:\n",
        "                                entry['port'] = ip_part[1]\n",
        "\n",
        "                    # Packet length detection\n",
        "                    elif re.match(r'(DgmLen:|Length:)\\s*\\d+', line, re.IGNORECASE):\n",
        "                        length_match = re.search(r'\\d+', line)\n",
        "                        if length_match:\n",
        "                            entry['packet_length'] = int(length_match.group())\n",
        "\n",
        "                    # Payload detection\n",
        "                    elif re.match(r'(PL\\(|Payload:).*([0-9a-f]{2}\\s*)+', line, re.IGNORECASE):\n",
        "                        hex_payload = re.findall(r'[0-9a-f]{2}', line.split(':')[-1])\n",
        "                        if hex_payload:\n",
        "                            entry['payload_sum'] = sum(int(byte, 16) for byte in hex_payload)\n",
        "                            entry['payload_len'] = len(hex_payload)\n",
        "\n",
        "                    # Protocol detection\n",
        "                    elif re.match(r'(Proto:|Protocol:)\\s*\\w+', line, re.IGNORECASE):\n",
        "                        proto_match = re.search(r'\\w+$', line)\n",
        "                        if proto_match:\n",
        "                            entry['protocol'] = proto_match.group().lower()\n",
        "\n",
        "                if entry:\n",
        "                    data.append(entry)\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        self._analyze_features(df)\n",
        "        return df\n",
        "\n",
        "    def _analyze_features(self, df):\n",
        "        \"\"\"Analyze available features and create processing pipeline\"\"\"\n",
        "        numeric_features = []\n",
        "        categorical_features = []\n",
        "\n",
        "        # Auto-detect feature types\n",
        "        for col in df.columns:\n",
        "            if pd.api.types.is_numeric_dtype(df[col]):\n",
        "                numeric_features.append(col)\n",
        "            elif col not in ['timestamp', 'src_mac', 'dst_mac', 'src_ip', 'dst_ip']:\n",
        "                # Try to convert to numeric\n",
        "                try:\n",
        "                    df[col] = pd.to_numeric(df[col])\n",
        "                    numeric_features.append(col)\n",
        "                except:\n",
        "                    if df[col].nunique() < 20:  # Consider low-cardinality as categorical\n",
        "                        categorical_features.append(col)\n",
        "\n",
        "        # IP address processing\n",
        "        ip_features = []\n",
        "        for col in ['src_ip', 'dst_ip']:\n",
        "            if col in df.columns:\n",
        "                ip_features.append(col)\n",
        "                df[f'{col}_num'] = df[col].apply(self._ip_to_num)\n",
        "                numeric_features.append(f'{col}_num')\n",
        "\n",
        "        # Fill missing values\n",
        "        numeric_defaults = {col: 0 for col in numeric_features}\n",
        "        df.fillna(numeric_defaults, inplace=True)\n",
        "\n",
        "        # For categorical features, use one-hot encoding\n",
        "        if categorical_features:\n",
        "            df = pd.get_dummies(df, columns=categorical_features)\n",
        "            numeric_features.extend([col for col in df.columns if col.endswith('_1')])\n",
        "\n",
        "        self.feature_columns = numeric_features\n",
        "        print(f\"Automatically selected features: {self.feature_columns}\")\n",
        "\n",
        "    def _ip_to_num(self, ip):\n",
        "        \"\"\"Convert IP address to numeric value\"\"\"\n",
        "        if pd.isna(ip):\n",
        "            return 0\n",
        "        try:\n",
        "            if ':' in ip:  # IPv6\n",
        "                return int(ip.replace(':', ''), 16)\n",
        "            else:  # IPv4\n",
        "                return sum(int(part) * (256 ** i) for i, part in enumerate(ip.split('.')[::-1]))\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    def _build_models(self, input_dim):\n",
        "        \"\"\"Build all autoencoder models\"\"\"\n",
        "        models = {\n",
        "            'AutoEncoder': self._build_autoencoder(input_dim),\n",
        "            'Sparse AutoEncoder': self._build_sparse_autoencoder(input_dim),\n",
        "            'Variational AutoEncoder': self._build_variational_autoencoder(input_dim),\n",
        "            'Stacked AutoEncoder': self._build_stacked_autoencoder(input_dim)\n",
        "        }\n",
        "        return models\n",
        "\n",
        "    def _build_autoencoder(self, input_dim):\n",
        "        \"\"\"Basic autoencoder\"\"\"\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoded = Dense(32, activation='relu')(input_layer)\n",
        "        decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "        return Model(input_layer, decoded)\n",
        "\n",
        "    def _build_sparse_autoencoder(self, input_dim):\n",
        "        \"\"\"Sparse autoencoder with L1 regularization\"\"\"\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoded = Dense(32, activation='relu',\n",
        "                       activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
        "        decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "        return Model(input_layer, decoded)\n",
        "\n",
        "    def _build_variational_autoencoder(self, input_dim):\n",
        "        \"\"\"Variational autoencoder\"\"\"\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        h = Dense(32, activation='relu')(input_layer)\n",
        "        z_mean = Dense(16)(h)\n",
        "        z_log_var = Dense(16)(h)\n",
        "\n",
        "        def sampling(args):\n",
        "            z_mean, z_log_var = args\n",
        "            epsilon = K.random_normal(shape=(K.shape(z_mean)[0], 16))\n",
        "            return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "\n",
        "        z = Lambda(sampling)([z_mean, z_log_var])\n",
        "        decoder_h = Dense(32, activation='relu')\n",
        "        decoder_mean = Dense(input_dim, activation='sigmoid')\n",
        "        h_decoded = decoder_h(z)\n",
        "        x_decoded_mean = decoder_mean(h_decoded)\n",
        "        return Model(input_layer, x_decoded_mean)\n",
        "\n",
        "    def _build_stacked_autoencoder(self, input_dim):\n",
        "        \"\"\"Stacked autoencoder\"\"\"\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoded = Dense(64, activation='relu')(input_layer)\n",
        "        encoded = Dense(32, activation='relu')(encoded)\n",
        "        decoded = Dense(64, activation='relu')(encoded)\n",
        "        decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "        return Model(input_layer, decoded)\n",
        "\n",
        "    def detect_anomalies(self, reconstructions, original):\n",
        "        \"\"\"Detect anomalies based on reconstruction error\"\"\"\n",
        "        anomaly_scores = np.mean(np.abs(original - reconstructions), axis=1)\n",
        "        threshold = np.percentile(anomaly_scores, self.threshold_percentile)\n",
        "        return anomaly_scores > threshold\n",
        "\n",
        "    def train(self, file_paths, epochs=50, batch_size=32):\n",
        "        \"\"\"Train the anomaly detection system\"\"\"\n",
        "        print(\"Parsing log files...\")\n",
        "        df = self.parse_logs(file_paths)\n",
        "\n",
        "        if not self.feature_columns:\n",
        "            raise ValueError(\"No features detected in the log files\")\n",
        "\n",
        "        X = df[self.feature_columns].values\n",
        "        print(\"Normalizing data...\")\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        print(\"\\nBuilding models...\")\n",
        "        self.models = self._build_models(X_scaled.shape[1])\n",
        "\n",
        "        results = {}\n",
        "        print(\"\\nTraining models...\")\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "            start_time = time.time()\n",
        "            model.fit(X_scaled, X_scaled,\n",
        "                     epochs=epochs,\n",
        "                     batch_size=batch_size,\n",
        "                     shuffle=True,\n",
        "                     validation_split=0.2,\n",
        "                     verbose=0)\n",
        "            training_time = time.time() - start_time\n",
        "\n",
        "            reconstructions = model.predict(X_scaled)\n",
        "            mse = mean_squared_error(X_scaled, reconstructions)\n",
        "            accuracy = 1 - mse\n",
        "            results[name] = {\n",
        "                'Accuracy': accuracy,\n",
        "                'MSE': mse,\n",
        "                'Training Time': training_time\n",
        "            }\n",
        "            print(f\"{name}: Accuracy={accuracy:.4f}, MSE={mse:.6f}, Training Time={training_time:.2f}s\")\n",
        "\n",
        "        # Select best model\n",
        "        self.best_model_name = max(results, key=lambda k: results[k]['Accuracy'])\n",
        "        self.best_model = self.models[self.best_model_name]\n",
        "        print(f\"\\nBest Model: {self.best_model_name} with Accuracy {results[self.best_model_name]['Accuracy']:.4f}\")\n",
        "\n",
        "        # Detect anomalies\n",
        "        print(\"Detecting anomalies...\")\n",
        "        reconstructions = self.best_model.predict(X_scaled)\n",
        "        df['Anomaly'] = self.detect_anomalies(reconstructions, X_scaled)\n",
        "        df['Anomaly_Score'] = np.mean(np.abs(X_scaled - reconstructions), axis=1)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def analyze_new_data(self, file_paths):\n",
        "        \"\"\"Analyze new log files using trained model\"\"\"\n",
        "        if not self.best_model:\n",
        "            raise ValueError(\"Model not trained yet. Call train() first.\")\n",
        "\n",
        "        print(\"Parsing new log files...\")\n",
        "        df = self.parse_logs(file_paths)\n",
        "\n",
        "        if not self.feature_columns:\n",
        "            raise ValueError(\"No features detected in the log files\")\n",
        "\n",
        "        # Ensure all expected features are present\n",
        "        missing_features = set(self.feature_columns) - set(df.columns)\n",
        "        for feat in missing_features:\n",
        "            df[feat] = 0  # Add missing features with default value\n",
        "\n",
        "        X = df[self.feature_columns].values\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "\n",
        "        print(\"Detecting anomalies...\")\n",
        "        reconstructions = self.best_model.predict(X_scaled)\n",
        "        df['Anomaly'] = self.detect_anomalies(reconstructions, X_scaled)\n",
        "        df['Anomaly_Score'] = np.mean(np.abs(X_scaled - reconstructions), axis=1)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def save_results(self, df, output_file='network_anomalies.csv'):\n",
        "        \"\"\"Save results to CSV file\"\"\"\n",
        "        # Include all original columns plus anomaly info\n",
        "        output_columns = [col for col in df.columns if not col.endswith('_num')]\n",
        "        df[output_columns].to_csv(output_file, index=False)\n",
        "        print(f\"\\nResults saved to '{output_file}'\")\n",
        "\n",
        "        # Show some detected anomalies\n",
        "        if 'Anomaly' in df.columns:\n",
        "            print(\"\\nSample anomalies detected:\")\n",
        "            print(df[df['Anomaly']][output_columns].head())\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize detector\n",
        "    detector = NetworkAnomalyDetector(threshold_percentile=95)\n",
        "\n",
        "    # Train on your log files (can be one or multiple)\n",
        "    log_files = [\"logsys1.txt\", \"logts1.txt\",\"logsnort1.txt\"]  # Replace with your files\n",
        "    df = detector.train(log_files)\n",
        "\n",
        "    # Save results\n",
        "    detector.save_results(df)\n",
        "\n",
        "    # Later, you can analyze new logs with the trained model\n",
        "    # new_logs = [\"new_logs.txt\"]\n",
        "    # new_results = detector.analyze_new_data(new_logs)\n",
        "    # detector.save_results(new_results, \"new_anomalies.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtOf7cBFpdNB",
        "outputId": "c7bdea97-592b-4b90-a8bc-e8555e782163"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing log files...\n",
            "Automatically selected features: ['src_port', 'dst_port', 'payload_sum', 'payload_len', 'src_ip_num', 'dst_ip_num']\n",
            "Normalizing data...\n",
            "\n",
            "Building models...\n",
            "\n",
            "Training models...\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "AutoEncoder: Accuracy=0.9997, MSE=0.000323, Training Time=29.95s\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Sparse AutoEncoder: Accuracy=0.9992, MSE=0.000757, Training Time=30.10s\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Variational AutoEncoder: Accuracy=0.9997, MSE=0.000322, Training Time=35.05s\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Stacked AutoEncoder: Accuracy=0.9999, MSE=0.000104, Training Time=33.02s\n",
            "\n",
            "Best Model: Stacked AutoEncoder with Accuracy 0.9999\n",
            "Detecting anomalies...\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\n",
            "Results saved to 'network_anomalies.csv'\n",
            "\n",
            "Sample anomalies detected:\n",
            "                   timestamp            src_mac            dst_mac  \\\n",
            "17  Thu Mar 20 23:55:21 2025  6a:61:ab:9d:ff:79  a0:59:50:93:56:70   \n",
            "24  Thu Mar 20 23:55:24 2025  6a:61:ab:9d:ff:79  ff:ff:ff:ff:ff:ff   \n",
            "25  Thu Mar 20 23:55:24 2025  a0:59:50:93:56:70  6a:61:ab:9d:ff:79   \n",
            "29  Thu Mar 20 23:55:27 2025  6a:61:ab:9d:ff:79  a0:59:50:93:56:70   \n",
            "42  Thu Mar 20 23:55:30 2025  6a:61:ab:9d:ff:79  a0:59:50:93:56:70   \n",
            "\n",
            "             src_ip  src_port           dst_ip  dst_port  payload_sum  \\\n",
            "17     104.18.26.48     443.0  192.168.135.162   54175.0          0.0   \n",
            "24  255.121.192.168       0.0       135.90.0.0       0.0       4620.0   \n",
            "25   86.112.192.168       0.0   135.162.106.97       0.0       4439.0   \n",
            "29     52.98.56.210     443.0  192.168.135.162   61673.0          0.0   \n",
            "42   192.168.135.90      53.0  192.168.135.162   49906.0      12189.0   \n",
            "\n",
            "    payload_len   ip port  Anomaly  Anomaly_Score  \n",
            "17          0.0  NaN  NaN     True       0.012176  \n",
            "24         60.0  NaN  NaN     True       0.027031  \n",
            "25         60.0  NaN  NaN     True       0.018274  \n",
            "29          0.0  NaN  NaN     True       0.012819  \n",
            "42        160.0  NaN  NaN     True       0.012481  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}